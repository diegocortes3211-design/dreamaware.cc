[
  {
    "title": "Attention Is All You Need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"],
    "year": 2017,
    "venue": "NIPS",
    "url": "https://arxiv.org/abs/1706.03762",
    "doi": "10.48550/arXiv.1706.03762",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    "keywords": ["transformer", "attention mechanism", "neural networks", "sequence modeling"]
  },
  {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
    "year": 2019,
    "venue": "NAACL-HLT",
    "url": "https://arxiv.org/abs/1810.04805",
    "doi": "10.48550/arXiv.1810.04805",
    "abstract": "We introduce BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
    "keywords": ["BERT", "bidirectional transformers", "language understanding", "pre-training"]
  }
]